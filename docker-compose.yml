
services:
  vllm-llm:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: vllm-llm-2
    ports:
      - "8001:8000"
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - MODEL=unsloth/Llama-3.2-3B-Instruct
      - PORT=8000
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model unsloth/Llama-3.2-3B-Instruct
      --max-model-len 8192
      --gpu-memory-utilization 0.5
      --enforce-eager
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --served-model-name gpt-3.5-turbo
    networks:
      - rag-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 120s

  vllm-embedding:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: vllm-embedding-2
    ports:
      - "8002:8000"
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
      - MODEL=jinaai/jina-embeddings-v4-vllm-retrieval
      - PORT=8000
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_USE_V1=0
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
      - TOKENIZERS_PARALLELISM=false
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model jinaai/jina-embeddings-v4-vllm-retrieval
      --max-model-len 8192
      --gpu-memory-utilization 0.25
      --enforce-eager
      --host 0.0.0.0
      --port 8000
      --task embed
      --trust-remote-code
      --dtype float16
      --disable-log-stats
      --served-model-name jina-embeddings-v4
      --disable-log-requests
    networks:
      - rag-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 120s

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant-2
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - rag-network

  nginx:
    image: nginx:alpine
    container_name: nginx-proxy-2
    ports:
      - "8000:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      vllm-llm:
        condition: service_healthy
      vllm-embedding:
        condition: service_healthy
    networks:
      - rag-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  app:
    build:
      context: ./app
      dockerfile: Dockerfile
    container_name: rag-app-2
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./app:/app
    depends_on:
      - qdrant
      - nginx
    networks:
      - rag-network
    environment:
      - OPENAI_API_KEY=sk-12345
      - OPENAI_API_BASE=http://nginx:80/v1

volumes:
  qdrant_data:

networks:
  rag-network:
    driver: bridge