events {
    worker_connections 1024;
}

http {
    upstream vllm_llm {
        server vllm-llm:8000;
    }

    upstream vllm_embedding {
        server vllm-embedding:8000;
    }

    upstream app {
        server app:5000;
    }

    server {
        listen 80;
        server_name localhost;

        # Increase client max body size for file uploads
        client_max_body_size 100M;

        # Health check endpoint
        location /health {
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }

        # Proxy for LLM service (Llama-3.2-3B-Instruct)
        location ~* ^/v1/(chat/)?completions.* {
            proxy_pass http://vllm_llm;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # WebSocket support
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            
            # Timeouts
            proxy_read_timeout 300s;
            proxy_send_timeout 300s;
            proxy_connect_timeout 75s;
        }

        # Proxy for embedding service (jina-embeddings-v4)
        location ~* ^/v1/embeddings.* {
            proxy_pass http://vllm_embedding;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeouts
            proxy_read_timeout 300s;
            proxy_send_timeout 300s;
            proxy_connect_timeout 75s;
        }

        # Catch-all for other OpenAI-compatible endpoints
        location /v1/ {
            # Default to LLM service
            proxy_pass http://vllm_llm;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Model listing endpoint
        location /v1/models {
            proxy_pass http://vllm_llm;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Root location - proxy to Flask app
        location / {
            proxy_pass http://app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}