# Custom vLLM Dockerfile for RAG project compatibility
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.11 python3.11-dev python3-pip \
    git curl cmake ninja-build gcc g++ \
    libopenblas-dev libomp-dev \
    && rm -rf /var/lib/apt/lists/*

# Create symbolic link for python
RUN ln -s /usr/bin/python3.11 /usr/bin/python

# Upgrade pip
RUN python -m pip install --upgrade pip

# Install PyTorch with CUDA support
RUN pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121

# Install latest vLLM for better compatibility
RUN pip install vllm --extra-index-url https://download.pytorch.org/whl/cu121

# Install additional dependencies for model compatibility
RUN pip install \
    transformers>=4.40.0 \
    accelerate>=0.27.0 \
    safetensors>=0.4.2 \
    sentencepiece>=0.1.99 \
    protobuf>=4.25.1 \
    fastapi>=0.104.1 \
    uvicorn>=0.24.0 \
    pydantic>=2.5.0 \
    numpy>=1.24.3 \
    scipy>=1.11.4 \
    scikit-learn>=1.3.2 \
    outlines

# Install specific dependencies for unsloth models
RUN pip install \
    unsloth \
    flash-attn>=2.5.6

# Install dependencies for BGE models
RUN pip install \
    sentence-transformers>=2.2.2 \
    huggingface-hub>=0.19.4

# Create working directory
WORKDIR /workspace

# Create a startup script
RUN echo '#!/bin/bash\n\
echo "Starting vLLM server..."\n\
echo "Model: $MODEL"\n\
echo "Port: $PORT"\n\
echo "Additional args: $@"\n\
\n\
# Start vLLM server with provided arguments\n\
exec python -m vllm.entrypoints.openai.api_server --model "$MODEL" --port "$PORT" "$@"\n\
' > /workspace/start.sh && chmod +x /workspace/start.sh

# Expose default port
EXPOSE 8000

# Set default command
ENTRYPOINT ["bash", "/workspace/start.sh"] 